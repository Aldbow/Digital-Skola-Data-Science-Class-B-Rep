{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction to AI.ipynb","provenance":[],"authorship_tag":"ABX9TyM2qLdboEVZJOAV5XoUJxv2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9nyPaHbC_ykm"},"source":["# 0. Install Dependencies"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYkrxpYL6vu8","executionInfo":{"status":"ok","timestamp":1638597529748,"user_tz":-420,"elapsed":10654,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"ebbdc7c3-8223-4e28-ab78-3e551118d916"},"source":["!pip install tensorflow==2.3.0\n","!pip install gym\n","!pip install keras\n","!pip install keras-rl2\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install -U colabgymrender"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting colabgymrender\n","  Downloading colabgymrender-1.0.9-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.62.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.18.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n","Installing collected packages: colabgymrender\n","Successfully installed colabgymrender-1.0.9\n"]}]},{"cell_type":"markdown","metadata":{"id":"gvpYWEe8_6XA"},"source":["# 1. Test Random Environment with OpenAI Gym"]},{"cell_type":"code","metadata":{"id":"01RiuoMm8ENm","executionInfo":{"status":"ok","timestamp":1638596677756,"user_tz":-420,"elapsed":338,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["import gym \n","import random"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"SDg04Q8X9yvT","executionInfo":{"status":"ok","timestamp":1638596678091,"user_tz":-420,"elapsed":2,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["env = gym.make('CartPole-v0')\n","states = env.observation_space.shape[0]\n","actions = env.action_space.n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFBjmYuy92RM","executionInfo":{"status":"ok","timestamp":1638596679357,"user_tz":-420,"elapsed":3,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"590b7688-c088-496f-a2ac-023717d62880"},"source":["actions"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlMX6xjd94kw","executionInfo":{"status":"ok","timestamp":1638596681471,"user_tz":-420,"elapsed":545,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"dea4e718-51bc-49a2-95ff-00fb046be22b"},"source":["episodes = 10\n","for episode in range(1, episodes+1):\n","    state = env.reset()\n","    done = False\n","    score = 0 \n","    \n","    while not done:\n","        env.render()\n","        action = random.choice([0,1])\n","        n_state, reward, done, info = env.step(action)\n","        score+=reward\n","    print('Episode:{} Score:{}'.format(episode, score))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode:1 Score:11.0\n","Episode:2 Score:22.0\n","Episode:3 Score:17.0\n","Episode:4 Score:22.0\n","Episode:5 Score:24.0\n","Episode:6 Score:16.0\n","Episode:7 Score:12.0\n","Episode:8 Score:23.0\n","Episode:9 Score:23.0\n","Episode:10 Score:44.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"63_9eOyf_-VF"},"source":["# 2. Create a Deep Learning Model with Keras"]},{"cell_type":"code","metadata":{"id":"mKeWTyDv94nL","executionInfo":{"status":"ok","timestamp":1638596824392,"user_tz":-420,"elapsed":354,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.optimizers import Adam"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"dr2y09Ug94pF","executionInfo":{"status":"ok","timestamp":1638596826779,"user_tz":-420,"elapsed":333,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["def build_model(states, actions):\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(1,states)))\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(actions, activation='linear'))\n","    return model"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYn5fjJ294rJ","executionInfo":{"status":"ok","timestamp":1638596828099,"user_tz":-420,"elapsed":3,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["model = build_model(states, actions)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MPy3ncQ94tF","executionInfo":{"status":"ok","timestamp":1638596829656,"user_tz":-420,"elapsed":4,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"d0b93bcb-17b7-4492-b4eb-c392c8e4f32d"},"source":["model.summary()"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_1 (Flatten)         (None, 4)                 0         \n","                                                                 \n"," dense_3 (Dense)             (None, 24)                120       \n","                                                                 \n"," dense_4 (Dense)             (None, 24)                600       \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 50        \n","                                                                 \n","=================================================================\n","Total params: 770\n","Trainable params: 770\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"r63qHh5RAB11"},"source":["# 3. Build Agent with Keras-RL"]},{"cell_type":"code","metadata":{"id":"4pxQDmIv-DF9","executionInfo":{"status":"ok","timestamp":1638596830978,"user_tz":-420,"elapsed":2,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["from rl.agents import DQNAgent\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hjrt0NL-EvJ","executionInfo":{"status":"ok","timestamp":1638596832347,"user_tz":-420,"elapsed":1,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["def build_agent(model, actions):\n","    policy = BoltzmannQPolicy()\n","    memory = SequentialMemory(limit=50000, window_length=1)\n","    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n","                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n","    return dqn"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcsKP2jn-H_I","executionInfo":{"status":"ok","timestamp":1638597210798,"user_tz":-420,"elapsed":376672,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"a89f9d1a-93df-4c46-9ae9-7d691ee41d27"},"source":["dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Training for 50000 steps ...\n","Interval 1 (0 steps performed)\n","\r    1/10000 [..............................] - ETA: 7:34 - reward: 1.0000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"]},{"output_type":"stream","name":"stdout","text":["10000/10000 [==============================] - 73s 7ms/step - reward: 1.0000\n","112 episodes - episode_reward: 89.018 [9.000, 200.000] - loss: 3.190 - mae: 19.606 - mean_q: 39.693\n","\n","Interval 2 (10000 steps performed)\n","10000/10000 [==============================] - 75s 8ms/step - reward: 1.0000\n","50 episodes - episode_reward: 198.560 [180.000, 200.000] - loss: 4.796 - mae: 39.248 - mean_q: 79.326\n","\n","Interval 3 (20000 steps performed)\n","10000/10000 [==============================] - 75s 7ms/step - reward: 1.0000\n","50 episodes - episode_reward: 198.840 [182.000, 200.000] - loss: 3.963 - mae: 39.909 - mean_q: 80.215\n","\n","Interval 4 (30000 steps performed)\n","10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n","53 episodes - episode_reward: 189.981 [144.000, 200.000] - loss: 5.040 - mae: 41.024 - mean_q: 82.349\n","\n","Interval 5 (40000 steps performed)\n","10000/10000 [==============================] - 77s 8ms/step - reward: 1.0000\n","done, took 376.001 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f60eed69150>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPyB0y1x-Jk0","executionInfo":{"status":"ok","timestamp":1638597248734,"user_tz":-420,"elapsed":13473,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"f7e08126-fc95-432d-cf07-ccd156ee4530"},"source":["scores = dqn.test(env, nb_episodes=100, visualize=False)\n","print(np.mean(scores.history['episode_reward']))"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 100 episodes ...\n","Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 200.000, steps: 200\n","Episode 3: reward: 198.000, steps: 198\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 200.000, steps: 200\n","Episode 6: reward: 188.000, steps: 188\n","Episode 7: reward: 200.000, steps: 200\n","Episode 8: reward: 200.000, steps: 200\n","Episode 9: reward: 200.000, steps: 200\n","Episode 10: reward: 200.000, steps: 200\n","Episode 11: reward: 182.000, steps: 182\n","Episode 12: reward: 195.000, steps: 195\n","Episode 13: reward: 200.000, steps: 200\n","Episode 14: reward: 200.000, steps: 200\n","Episode 15: reward: 200.000, steps: 200\n","Episode 16: reward: 200.000, steps: 200\n","Episode 17: reward: 187.000, steps: 187\n","Episode 18: reward: 183.000, steps: 183\n","Episode 19: reward: 200.000, steps: 200\n","Episode 20: reward: 188.000, steps: 188\n","Episode 21: reward: 200.000, steps: 200\n","Episode 22: reward: 196.000, steps: 196\n","Episode 23: reward: 200.000, steps: 200\n","Episode 24: reward: 200.000, steps: 200\n","Episode 25: reward: 192.000, steps: 192\n","Episode 26: reward: 196.000, steps: 196\n","Episode 27: reward: 200.000, steps: 200\n","Episode 28: reward: 200.000, steps: 200\n","Episode 29: reward: 200.000, steps: 200\n","Episode 30: reward: 180.000, steps: 180\n","Episode 31: reward: 200.000, steps: 200\n","Episode 32: reward: 200.000, steps: 200\n","Episode 33: reward: 176.000, steps: 176\n","Episode 34: reward: 199.000, steps: 199\n","Episode 35: reward: 200.000, steps: 200\n","Episode 36: reward: 200.000, steps: 200\n","Episode 37: reward: 200.000, steps: 200\n","Episode 38: reward: 185.000, steps: 185\n","Episode 39: reward: 200.000, steps: 200\n","Episode 40: reward: 188.000, steps: 188\n","Episode 41: reward: 200.000, steps: 200\n","Episode 42: reward: 200.000, steps: 200\n","Episode 43: reward: 197.000, steps: 197\n","Episode 44: reward: 191.000, steps: 191\n","Episode 45: reward: 191.000, steps: 191\n","Episode 46: reward: 200.000, steps: 200\n","Episode 47: reward: 188.000, steps: 188\n","Episode 48: reward: 200.000, steps: 200\n","Episode 49: reward: 181.000, steps: 181\n","Episode 50: reward: 200.000, steps: 200\n","Episode 51: reward: 178.000, steps: 178\n","Episode 52: reward: 200.000, steps: 200\n","Episode 53: reward: 195.000, steps: 195\n","Episode 54: reward: 184.000, steps: 184\n","Episode 55: reward: 200.000, steps: 200\n","Episode 56: reward: 200.000, steps: 200\n","Episode 57: reward: 200.000, steps: 200\n","Episode 58: reward: 200.000, steps: 200\n","Episode 59: reward: 200.000, steps: 200\n","Episode 60: reward: 200.000, steps: 200\n","Episode 61: reward: 168.000, steps: 168\n","Episode 62: reward: 200.000, steps: 200\n","Episode 63: reward: 200.000, steps: 200\n","Episode 64: reward: 200.000, steps: 200\n","Episode 65: reward: 200.000, steps: 200\n","Episode 66: reward: 200.000, steps: 200\n","Episode 67: reward: 200.000, steps: 200\n","Episode 68: reward: 200.000, steps: 200\n","Episode 69: reward: 170.000, steps: 170\n","Episode 70: reward: 200.000, steps: 200\n","Episode 71: reward: 198.000, steps: 198\n","Episode 72: reward: 200.000, steps: 200\n","Episode 73: reward: 200.000, steps: 200\n","Episode 74: reward: 200.000, steps: 200\n","Episode 75: reward: 200.000, steps: 200\n","Episode 76: reward: 187.000, steps: 187\n","Episode 77: reward: 200.000, steps: 200\n","Episode 78: reward: 200.000, steps: 200\n","Episode 79: reward: 182.000, steps: 182\n","Episode 80: reward: 200.000, steps: 200\n","Episode 81: reward: 180.000, steps: 180\n","Episode 82: reward: 200.000, steps: 200\n","Episode 83: reward: 200.000, steps: 200\n","Episode 84: reward: 200.000, steps: 200\n","Episode 85: reward: 193.000, steps: 193\n","Episode 86: reward: 200.000, steps: 200\n","Episode 87: reward: 200.000, steps: 200\n","Episode 88: reward: 191.000, steps: 191\n","Episode 89: reward: 200.000, steps: 200\n","Episode 90: reward: 200.000, steps: 200\n","Episode 91: reward: 200.000, steps: 200\n","Episode 92: reward: 200.000, steps: 200\n","Episode 93: reward: 200.000, steps: 200\n","Episode 94: reward: 200.000, steps: 200\n","Episode 95: reward: 200.000, steps: 200\n","Episode 96: reward: 195.000, steps: 195\n","Episode 97: reward: 200.000, steps: 200\n","Episode 98: reward: 200.000, steps: 200\n","Episode 99: reward: 200.000, steps: 200\n","Episode 100: reward: 198.000, steps: 198\n","196.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gbLptGdW-Lp1","executionInfo":{"status":"ok","timestamp":1638597257337,"user_tz":-420,"elapsed":5987,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"210465e5-07fc-4e73-8a0e-b2e5ed474065"},"source":["_ = dqn.test(env, nb_episodes=15, visualize=True)"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 15 episodes ...\n","Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 200.000, steps: 200\n","Episode 3: reward: 170.000, steps: 170\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 200.000, steps: 200\n","Episode 6: reward: 200.000, steps: 200\n","Episode 7: reward: 200.000, steps: 200\n","Episode 8: reward: 200.000, steps: 200\n","Episode 9: reward: 200.000, steps: 200\n","Episode 10: reward: 173.000, steps: 173\n","Episode 11: reward: 196.000, steps: 196\n","Episode 12: reward: 177.000, steps: 177\n","Episode 13: reward: 200.000, steps: 200\n","Episode 14: reward: 200.000, steps: 200\n","Episode 15: reward: 200.000, steps: 200\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mt4KuhCu_uXo"},"source":["# 4. Reloading Agent from Memory"]},{"cell_type":"code","metadata":{"id":"m12RJ9a--NFd","executionInfo":{"status":"ok","timestamp":1638597277268,"user_tz":-420,"elapsed":348,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["dqn.save_weights('dqn_weights.h5f', overwrite=True)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0TPlKKM-OLU","executionInfo":{"status":"ok","timestamp":1638597293791,"user_tz":-420,"elapsed":356,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["del model\n","del dqn\n","del env"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSfF7Jng-Pn9","executionInfo":{"status":"ok","timestamp":1638597295708,"user_tz":-420,"elapsed":570,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"6a595368-dcf7-404a-b9eb-4a3843cf38ab"},"source":["env = gym.make('CartPole-v0')\n","actions = env.action_space.n\n","states = env.observation_space.shape[0]\n","model = build_model(states, actions)\n","dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"code","metadata":{"id":"gEsrvM-2_oyj","executionInfo":{"status":"ok","timestamp":1638597297738,"user_tz":-420,"elapsed":345,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}}},"source":["dqn.load_weights('dqn_weights.h5f')"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TP3noI6_qLn","executionInfo":{"status":"ok","timestamp":1638597301273,"user_tz":-420,"elapsed":2379,"user":{"displayName":"Agil Haykal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmAfVlWdi3HIJ21pC5ArTZL0YwdYJDAgw8HDWv=s64","userId":"02021994300640528465"}},"outputId":"c48b2ba5-3284-42b0-c1d4-67fd8b421361"},"source":["_ = dqn.test(env, nb_episodes=5, visualize=True)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 5 episodes ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 196.000, steps: 196\n","Episode 3: reward: 200.000, steps: 200\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 177.000, steps: 177\n"]}]},{"cell_type":"code","metadata":{"id":"Qx9g_-pbAohG"},"source":[""],"execution_count":null,"outputs":[]}]}